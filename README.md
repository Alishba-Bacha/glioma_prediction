# Empowering Glioma Prognosis With Transparent Machine  Learning and Interpretative Insights Using Explainable AI
The lack of interpretability in AI models continues to be a major obstacle to their widespread adoption, even with the advancements in machine learning applications in healthcare. Improving user confidence and decision-making skills requires the creation of tools that offer concise, intelligible justifications for model predictions.
The goal of this project is to develop a machine learning tool that gives medical practitioners lucid insights into patient data. The problem stems from a lack of understanding and confidence in AI systems, which could lead to bad decisions. In order to help users visualize and comprehend model predictions, the approach comprises creating an interactive application that makes use of explainable AI techniques. A user-friendly tool that improves decision-making in healthcare settings and eventually improves patient outcomes is one of the expected outcomes. 
